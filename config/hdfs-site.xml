<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <!-- 数据块默认大小128M -->
        <name>dfs.block.size</name>
        <value>134217728</value>
    </property>
    <property>
        <!-- 控制目录扫描线程每秒最大运行时长，值越大扫描速度越快，但可能影响正常I/O -->
        <name>dfs.datanode.directoryscan.throttle.limit.ms.per.sec</name>
        <value>500</value>
    </property>
    <property>
        <!-- 默认6小时 -->
        <name>dfs.datanode.directoryscan.interval</name>
        <value>21600000</value>
    </property>
    <property>
        <!-- 并发扫描线程数 -->
        <name>dfs.datanode.directoryscan.threads</name>
        <value>2</value>
    </property>
    <property>
        <!-- 指定hdfs保存数据副本的数量，包括自己，默认为3 -->
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <!-- namenode节点数据（元数据）的存放位置，可以指定多个目录实现容错，用逗号分隔 -->
        <name>dfs.namenode.name.dir</name>
        <value>${HADOOP_HOME}/data/hdfs/namenode/data</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>${HADOOP_HOME}/data/hdfs/datanode/data1,${HADOOP_HOME}/data/hdfs/datanode/data2,${HADOOP_HOME}/data/hdfs/datanode/data3</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>4096</value>
    </property>
    <!-- 指定 HDFS 的 nameservice 为 hacluster，需要和 core-site.xml 中的保持一致 -->
    <!-- HA 集群的名称，值可以为任何值，下面是使用 hacluster，地方配置为自定义的值即可 -->
    <property>
        <name>dfs.nameservices</name>
        <value>hacluster</value>
    </property>
    <!-- NameNode 节点的名称，nn1 和 nn2，可自定义名称 -->
    <property>
        <name>dfs.ha.namenodes.hacluster</name>
        <value>nn1,nn2</value>
    </property>
    <!-- nn1的rpc、servicepc和http通信 -->
    <!-- NameNode 节点所在服务器，名称必须与上面定义的值对应 -->
    <property>
        <name>dfs.namenode.rpc-address.hacluster.nn1</name>
        <value>${ACTIVE_NAME_NODE}:8020</value>
    </property>
    <property>
        <name>dfs.namenode.servicepc-address.hacluster.nn1</name>
        <value>${ACTIVE_NAME_NODE}:53310</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.hacluster.nn1</name>
        <value>${ACTIVE_NAME_NODE}:50070</value>
    </property>
    <!-- nn2的rpc、servicepc和http通信 -->
    <!-- NameNode 节点所在服务器，名称必须与上面定义的值对应 -->
    <property>
        <name>dfs.namenode.rpc-address.hacluster.nn2</name>
        <value>${STANDBY_NAME_NODE}:8020</value>
    </property>
    <property>
        <name>dfs.namenode.servicepc-address.hacluster.nn2</name>
        <value>${STANDBY_NAME_NODE}:53310</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.hacluster.nn2</name>
        <value>${STANDBY_NAME_NODE}:50070</value>
    </property>
    <property>
        <!-- 指定namenode的元数据在JournalNode上存放的位置 -->
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://${JOURNAL_NODE}/hacluster</value>
    </property>
    <property>
        <name>dfs.hosts</name>
        <value>${HADOOP_HOME}/etc/hadoop/datanodes</value>
    </property>
    <property>
        <!-- 指定namenode的元数据在JournalNode上存放的位置 -->
        <name>dfs.journalnode.edits.dir</name>
        <value>${HADOOP_HOME}/data/hdfs/journalnode</value>
    </property>
    <property>
        <!-- namenode操作日志的存放位置 -->
        <name>dfs.namenode.edits.dir</name>
        <value>${HADOOP_HOME}/data/hdfs/namenode/edits</value>
    </property>
    <property>
        <!-- 开启namenode故障转移自动切换 -->
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <!-- 配置失败自动切换实现方式 -->
        <name>dfs.client.failover.proxy-provider.hacluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <!-- 配置隔离机制 -->
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <!-- 使用隔离机制需要SSH免密登录 -->
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/yjrszcq/.ssh/id_rsa</value>
    </property>
    <property>
        <!-- 设置hdfs操作权限，false表示任何用户都可以在hdfs上操作文件 -->
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
</configuration>
